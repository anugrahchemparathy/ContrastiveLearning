{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from versatileorbits import *\n",
    "#from RealNVP import *\n",
    "import masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVPNode(nn.Module):\n",
    "    def __init__(self, mask, hidden_size):\n",
    "        super(RealNVPNode, self).__init__()\n",
    "        self.dim = len(mask)\n",
    "        self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "\n",
    "        self.s_func = nn.Sequential(nn.Linear(in_features=self.dim, out_features=hidden_size), nn.LeakyReLU(),\n",
    "                                    nn.Linear(in_features=hidden_size, out_features=hidden_size), nn.LeakyReLU(),\n",
    "                                    nn.Linear(in_features=hidden_size, out_features=self.dim))\n",
    "\n",
    "        self.scale = nn.Parameter(torch.Tensor(self.dim))\n",
    "\n",
    "        self.t_func = nn.Sequential(nn.Linear(in_features=self.dim, out_features=hidden_size), nn.LeakyReLU(),\n",
    "                                    nn.Linear(in_features=hidden_size, out_features=hidden_size), nn.LeakyReLU(),\n",
    "                                    nn.Linear(in_features=hidden_size, out_features=self.dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_mask = x*self.mask\n",
    "        s = self.s_func(x_mask) * self.scale\n",
    "        t = self.t_func(x_mask)\n",
    "\n",
    "        y = x_mask + (1 - self.mask) * (x*torch.exp(s) + t)\n",
    "\n",
    "        # Sum for -1, since for every batch, and 1-mask, since the log_det_jac is 1 for y1:d = x1:d.\n",
    "        log_det_jac = ((1 - self.mask) * s).sum(-1)\n",
    "        return y, log_det_jac\n",
    "\n",
    "    def inverse(self, y):\n",
    "        print(self.mask.shape)\n",
    "        print(y.shape)\n",
    "        y_mask = y * self.mask\n",
    "        s = self.s_func(y_mask) * self.scale\n",
    "        t = self.t_func(y_mask)\n",
    "\n",
    "        x = y_mask + (1-self.mask)*(y - t)*torch.exp(-s)\n",
    "\n",
    "        inv_log_det_jac = ((1 - self.mask) * -s).sum(-1)\n",
    "\n",
    "        return x, inv_log_det_jac\n",
    "\n",
    "\n",
    "class RealNVP(nn.Module):\n",
    "    def __init__(self, masks, hidden_size):\n",
    "        super(RealNVP, self).__init__()\n",
    "\n",
    "        self.dim = len(masks[0])\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.masks = nn.ParameterList([nn.Parameter(torch.Tensor(mask), requires_grad=False) for mask in masks])\n",
    "        self.layers = nn.ModuleList([RealNVPNode(mask, self.hidden_size) for mask in self.masks])\n",
    "\n",
    "        self.distribution = MultivariateNormal(torch.zeros(self.dim), torch.eye(self.dim))\n",
    "\n",
    "    def log_probability(self, x):\n",
    "        log_prob = torch.zeros(x.shape[0])\n",
    "        for layer in reversed(self.layers):\n",
    "            x, inv_log_det_jac = layer.inverse(x)\n",
    "            log_prob += inv_log_det_jac\n",
    "        log_prob += self.distribution.log_prob(x)\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "    def rsample(self, num_samples):\n",
    "        x = self.distribution.sample((num_samples,))\n",
    "        log_prob = self.distribution.log_prob(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x, log_det_jac = layer.forward(x)\n",
    "            log_prob += log_det_jac\n",
    "\n",
    "        return x, log_prob\n",
    "\n",
    "    def sample_each_step(self, num_samples):\n",
    "        samples = []\n",
    "\n",
    "        x = self.distribution.sample((num_samples,))\n",
    "        samples.append(x.detach().numpy())\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x, _ = layer.forward(x)\n",
    "            samples.append(x.detach().numpy())\n",
    "\n",
    "        return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very simple training loop\n",
    "def train(model, num_epochs = 100, batch_size = 64):\n",
    "    NF_dataset = OrbitsDataset_NF(num_samples = 1000, phi0 = 1, H = -0.3, L = 0.5)\n",
    "    train_loader = torch.utils.data.DataLoader(NF_dataset, batch_size=batch_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in tqdm(range(num_epochs)):    \n",
    "        epoch_loss = 0\n",
    "        for orbit_position in train_loader:\n",
    "            log_probability = model.log_probability(orbit_position) #(batch_size)\n",
    "            loss = - torch.mean(log_probability, dim = 0)\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            \n",
    "        \n",
    "        epoch_loss /= len(train_loader)\n",
    "        losses.append(epoch_loss.detach())\n",
    "    \n",
    "    return model, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers= 4\n",
    "masks = masks.mask2(input_output_size = 4, num_layers = num_layers)\n",
    "hidden_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.92195445  1.66666667  1.         -0.3         0.5       ]\n",
      "torch.Size([1000, 4])\n",
      "It took 0.005406856536865234 time to finish the job.\n",
      "torch.Size([4])\n",
      "torch.Size([64, 4])\n",
      "torch.Size([4])\n",
      "torch.Size([64, 4])\n",
      "torch.Size([4])\n",
      "torch.Size([64, 4])\n",
      "torch.Size([4])\n",
      "torch.Size([64, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected value argument (Tensor of shape (64, 4)) to be within the support (IndependentConstraint(Real(), 1)) of the distribution MultivariateNormal(loc: torch.Size([4]), covariance_matrix: torch.Size([4, 4])), but found invalid values:\ntensor([[-1.3766, -2.6055,     inf,     nan],\n        [ 0.2442, -0.7375,     inf,  1.5533],\n        [-1.7779, -2.2471,    -inf,     nan],\n        [-1.0293, -2.4765,     inf,     nan],\n        [-0.4612, -1.9820,     inf,     nan],\n        [-1.6706, -2.5040,     inf,     nan],\n        [-1.0524, -0.4670,    -inf,     nan],\n        [-1.3049, -2.5938,     inf,     nan],\n        [-0.8901, -0.2716,    -inf,     nan],\n        [-0.8724, -2.3687,     inf,     nan],\n        [-1.6938, -1.5370,    -inf,     nan],\n        [-1.6399, -2.5338,     inf,     nan],\n        [-0.9675, -0.3650,    -inf,     nan],\n        [-1.7279, -1.6503,    -inf,     nan],\n        [-0.5030, -2.0249,     inf,     nan],\n        [-1.7653, -2.3119,     inf,     nan],\n        [-1.5143, -2.5967,     inf,     nan],\n        [-0.7981, -2.3093,     inf,     nan],\n        [-1.5476, -1.1969,    -inf,     nan],\n        [-1.7580, -1.7859,    -inf,     nan],\n        [-1.6397, -1.3892,    -inf,     nan],\n        [ 0.3186, -0.3903,     inf,  2.1158],\n        [-1.2210, -0.6899,    -inf,     nan],\n        [ 0.2325, -0.7770,     inf,     nan],\n        [-1.4044, -2.6072,     inf,     nan],\n        [-1.1166, -0.5504,    -inf,     nan],\n        [ 0.1051, -1.1269,     inf,     nan],\n        [ 0.0429, -1.2590,     inf,     nan],\n        [-1.0033, -0.4079,    -inf,     nan],\n        [-1.1890, -0.6470,    -inf,     nan],\n        [-0.5973,  0.0421,    -inf,     nan],\n        [-1.5993, -2.5627,     inf,     nan],\n        [ 0.1863, -0.9219,     inf,     nan],\n        [-1.7846, -2.0045,    -inf,     nan],\n        [-1.1833, -0.6394,    -inf,     nan],\n        [-0.2298, -1.7122,     inf,     nan],\n        [-0.9103, -0.2956,    -inf,     nan],\n        [-1.4834, -2.6027,     inf,     nan],\n        [-1.5114, -2.5974,     inf,     nan],\n        [-1.7239, -1.6362,    -inf,     nan],\n        [ 0.2093, -0.8536,     inf,     nan],\n        [-1.6132, -2.5539,     inf,     nan],\n        [-1.6371, -1.3833,    -inf,     nan],\n        [-0.9012, -0.2848,    -inf,     nan],\n        [-1.3779, -2.6056,     inf,     nan],\n        [-1.7225, -1.6311,    -inf,     nan],\n        [-0.4819, -2.0034,     inf,     nan],\n        [-1.0319, -0.4421,    -inf,     nan],\n        [-0.7196, -2.2376,     inf,     nan],\n        [-0.3009, -1.8035,     inf,     nan],\n        [-1.5682, -2.5785,     inf,     nan],\n        [-0.8249, -0.2006,    -inf,     nan],\n        [ 0.3171, -0.0735,    -inf,  2.8707],\n        [-1.6499, -2.5250,     inf,     nan],\n        [-0.0234, -1.3912,     inf,     nan],\n        [-0.7482, -2.2644,     inf,     nan],\n        [-1.6892, -2.4813,     inf,     nan],\n        [-1.7881, -2.1086,    -inf,     nan],\n        [-1.2741, -2.5860,     inf,     nan],\n        [-1.7528, -1.7570,    -inf,     nan],\n        [-1.7839, -2.1975,    -inf,     nan],\n        [-1.4500, -2.6064,     inf,     nan],\n        [-0.4091, -1.9276,     inf,     nan],\n        [-1.7799, -1.9532,    -inf,     nan]], grad_fn=<AddBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cca20964b659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRealNVP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6ffcabcee06c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0morbit_position\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mlog_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morbit_position\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7bf3ccc89880>\u001b[0m in \u001b[0;36mlog_probability\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_log_det_jac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mlog_prob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minv_log_det_jac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_batch_mahalanobis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             raise ValueError(\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0;34m\"Expected value argument \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m                 \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;34mf\"to be within the support ({repr(support)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected value argument (Tensor of shape (64, 4)) to be within the support (IndependentConstraint(Real(), 1)) of the distribution MultivariateNormal(loc: torch.Size([4]), covariance_matrix: torch.Size([4, 4])), but found invalid values:\ntensor([[-1.3766, -2.6055,     inf,     nan],\n        [ 0.2442, -0.7375,     inf,  1.5533],\n        [-1.7779, -2.2471,    -inf,     nan],\n        [-1.0293, -2.4765,     inf,     nan],\n        [-0.4612, -1.9820,     inf,     nan],\n        [-1.6706, -2.5040,     inf,     nan],\n        [-1.0524, -0.4670,    -inf,     nan],\n        [-1.3049, -2.5938,     inf,     nan],\n        [-0.8901, -0.2716,    -inf,     nan],\n        [-0.8724, -2.3687,     inf,     nan],\n        [-1.6938, -1.5370,    -inf,     nan],\n        [-1.6399, -2.5338,     inf,     nan],\n        [-0.9675, -0.3650,    -inf,     nan],\n        [-1.7279, -1.6503,    -inf,     nan],\n        [-0.5030, -2.0249,     inf,     nan],\n        [-1.7653, -2.3119,     inf,     nan],\n        [-1.5143, -2.5967,     inf,     nan],\n        [-0.7981, -2.3093,     inf,     nan],\n        [-1.5476, -1.1969,    -inf,     nan],\n        [-1.7580, -1.7859,    -inf,     nan],\n        [-1.6397, -1.3892,    -inf,     nan],\n        [ 0.3186, -0.3903,     inf,  2.1158],\n        [-1.2210, -0.6899,    -inf,     nan],\n        [ 0.2325, -0.7770,     inf,     nan],\n        [-1.4044, -2.6072,     inf,     nan],\n        [-1.1166, -0.5504,    -inf,     nan],\n        [ 0.1051, -1.1269,     inf,     nan],\n        [ 0.0429, -1.2590,     inf,     nan],\n        [-1.0033, -0.4079,    -inf,     nan],\n        [-1.1890, -0.6470,    -inf,     nan],\n        [-0.5973,  0.0421,    -inf,     nan],\n        [-1.5993, -2.5627,     inf,     nan],\n        [ 0.1863, -0.9219,     inf,     nan],\n        [-1.7846, -2.0045,    -inf,     nan],\n        [-1.1833, -0.6394,    -inf,     nan],\n        [-0.2298, -1.7122,     inf,     nan],\n        [-0.9103, -0.2956,    -inf,     nan],\n        [-1.4834, -2.6027,     inf,     nan],\n        [-1.5114, -2.5974,     inf,     nan],\n        [-1.7239, -1.6362,    -inf,     nan],\n        [ 0.2093, -0.8536,     inf,     nan],\n        [-1.6132, -2.5539,     inf,     nan],\n        [-1.6371, -1.3833,    -inf,     nan],\n        [-0.9012, -0.2848,    -inf,     nan],\n        [-1.3779, -2.6056,     inf,     nan],\n        [-1.7225, -1.6311,    -inf,     nan],\n        [-0.4819, -2.0034,     inf,     nan],\n        [-1.0319, -0.4421,    -inf,     nan],\n        [-0.7196, -2.2376,     inf,     nan],\n        [-0.3009, -1.8035,     inf,     nan],\n        [-1.5682, -2.5785,     inf,     nan],\n        [-0.8249, -0.2006,    -inf,     nan],\n        [ 0.3171, -0.0735,    -inf,  2.8707],\n        [-1.6499, -2.5250,     inf,     nan],\n        [-0.0234, -1.3912,     inf,     nan],\n        [-0.7482, -2.2644,     inf,     nan],\n        [-1.6892, -2.4813,     inf,     nan],\n        [-1.7881, -2.1086,    -inf,     nan],\n        [-1.2741, -2.5860,     inf,     nan],\n        [-1.7528, -1.7570,    -inf,     nan],\n        [-1.7839, -2.1975,    -inf,     nan],\n        [-1.4500, -2.6064,     inf,     nan],\n        [-0.4091, -1.9276,     inf,     nan],\n        [-1.7799, -1.9532,    -inf,     nan]], grad_fn=<AddBackward0>)"
     ]
    }
   ],
   "source": [
    "model = RealNVP(masks, hidden_size)\n",
    "\n",
    "model, losses = train(model, num_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
